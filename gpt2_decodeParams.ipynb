{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DialGPT - Conversational Chatbot on GPT2\n",
    "\n",
    "This notebook explores different decoding strategies.\n",
    "\n",
    "TODO:\n",
    "\n",
    "- [x] Greedy Search\n",
    "- [x] Beam Search\n",
    "- [x] Contrastive Search (in other cloud)\n",
    "- [x] Manipulating Logits Distributions (in other notebooks)\n",
    "\n",
    "A useful exercise to keep in mind:\n",
    "\t•\tText generation relative to the user’s interest. For example, if the user is engaged in the conversation, the chat generates longer responses (leading to more frequent messages). Otherwise, if the user is less interested, the responses are shorter.”\n",
    "\n",
    "Git Rep:\n",
    "https://github.com/microsoft/DialoGPT?tab=readme-ov-file\n",
    "\n",
    "HF Model Card:\n",
    "https://huggingface.co/microsoft/DialoGPT-small\n",
    "\n",
    "\n",
    "TODO:\n",
    "- [ ] Preform more statistical analysis on the correlation between decoders and expression states of generative models. \n",
    "\n",
    "Good sources\n",
    "- [Generative Aspect-Based Sentiment Analysis with Contrastive Learning and Expressive Structure](https://arxiv.org/pdf/2211.07743)\n",
    "- [Generative Sentiment Analysis via Latent Category Distribution and Constrained Decoding](https://arxiv.org/pdf/2407.21560)\n",
    "- [Bert Basics](https://arxiv.org/pdf/1911.00536)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "\n",
    "The following section includes exploratory analysis on `classes` and `scripts` to configure key components in controlling streaming chatbots during live sessions. The experiment was setup to fetch user based feature classes for e.g. their mood per text sequence (positive, negative or neutral). Further work has been done in analytical notebooks under Naomi's git where TFIF (Term frequency) was used upon the categories rather than embedding the dialogue text. This is useful in the sense of approximating a kernel feature space relative to the certain type of individual - a valuable feature representation. This is useful in the sense that you are able to interpret the policy function in an agent's behaviour (which is predominant in their speech expression) as a latent space or hyperplane. In other words, a linear regression but you are viewing it form another angle - on 2D grid. \n",
    "\n",
    "Brief summary of setup \n",
    "- **Chat Session**: Stores/appends speech pairs in a chat session.\n",
    "- **Generator**: GPT pipeline to return responses.\n",
    "- **Online Chain**: A chain of classifiers for feature extraction in dialogue.\n",
    "- **Experiment Sampler**: For experimenting with parameters like BeamSearch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More self-notes but related to building my current agent, Naomi\n",
    "\n",
    "Initially, the plan was to use two LLMs: one for roleplaying (e.g., Instruct LLaMA) and another for task-specific chats (e.g., DialGPT). The second model, DialGPT, is adjusted by decoding parameters inferred from chat sessions. Over time, these parameters are fine-tuned as more data is collected, with a threshold to maximize synthetic dataset generation. This fine-tuning may require significant compute resources, which may be postponed if needed.\n",
    "\n",
    "For Naomi, this approach can be delayed as more features need to be collected outside Naomi's current use case.\n",
    "\n",
    "### Reasoning:\n",
    "Engagement levels are used to model human-like behavior in chat dialogues, such as predicting \"double-texting\" or when the bot sends multiple responses. This helps predict when the bot won't stop generating responses.\n",
    "\n",
    "### Control Variables (Unit of Time):\n",
    "1. **`time_window`**: Counts the number of speech pairs (user, agent) in a chat session.\n",
    "2. **`session_window`**: Fine-tunes the neural network (e.g., LLaMA or ChatGPT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed, GenerationConfig\n",
    "\n",
    "set_seed(42)\n",
    "# history = []\n",
    "# user_role = 'user'\n",
    "# agent_role = 'assistant'\n",
    "model_card = \"microsoft/DialoGPT-small\"\n",
    "generator = pipeline('text-generation', model=model_card, device='mps')\n",
    "generator.tokenizer.pad_token_id = generator.tokenizer.eos_token_id \n",
    "generator.padding_side = 'left'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{% for message in messages %}{{ message.content }}{{ eos_token }}{% endfor %}'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.tokenizer.chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bos_token',\n",
       " 'eos_token',\n",
       " 'unk_token',\n",
       " 'sep_token',\n",
       " 'pad_token',\n",
       " 'cls_token',\n",
       " 'mask_token',\n",
       " 'additional_special_tokens']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.tokenizer.SPECIAL_TOKENS_ATTRIBUTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"My moms middle daughter's name\"}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(generator.tokenizer.apply_chat_template(\n",
    "\t[{'role': 'user', 'content': 'hey how are you?'}], tokenize=False, add_special_tokens=False\n",
    "), return_full_text=False, max_new_tokens=30, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'greedy': {'max_length': 20,\n",
       "  'max_new_tokens': 50,\n",
       "  'min_length': 0,\n",
       "  'min_new_tokens': None,\n",
       "  'early_stopping': False,\n",
       "  'max_time': None,\n",
       "  'stop_strings': None,\n",
       "  'do_sample': False,\n",
       "  'num_beams': 1,\n",
       "  'num_beam_groups': 1,\n",
       "  'penalty_alpha': None,\n",
       "  'dola_layers': None,\n",
       "  'use_cache': False,\n",
       "  'cache_implementation': None,\n",
       "  'cache_config': None,\n",
       "  'return_legacy_cache': None,\n",
       "  'temperature': 1.0,\n",
       "  'top_k': 50,\n",
       "  'top_p': 1.0,\n",
       "  'min_p': None,\n",
       "  'typical_p': 1.0,\n",
       "  'epsilon_cutoff': 0.0,\n",
       "  'eta_cutoff': 0.0,\n",
       "  'diversity_penalty': 0.0,\n",
       "  'repetition_penalty': 1.0,\n",
       "  'encoder_repetition_penalty': 1.0,\n",
       "  'length_penalty': 1.0,\n",
       "  'no_repeat_ngram_size': 0,\n",
       "  'bad_words_ids': None,\n",
       "  'force_words_ids': None,\n",
       "  'renormalize_logits': False,\n",
       "  'constraints': None,\n",
       "  'forced_bos_token_id': None,\n",
       "  'forced_eos_token_id': None,\n",
       "  'remove_invalid_values': False,\n",
       "  'exponential_decay_length_penalty': None,\n",
       "  'suppress_tokens': None,\n",
       "  'begin_suppress_tokens': None,\n",
       "  'forced_decoder_ids': None,\n",
       "  'sequence_bias': None,\n",
       "  'token_healing': False,\n",
       "  'guidance_scale': None,\n",
       "  'low_memory': None,\n",
       "  'watermarking_config': None,\n",
       "  'num_return_sequences': 1,\n",
       "  'output_attentions': False,\n",
       "  'output_hidden_states': False,\n",
       "  'output_scores': False,\n",
       "  'output_logits': None,\n",
       "  'return_dict_in_generate': False,\n",
       "  'pad_token_id': None,\n",
       "  'bos_token_id': None,\n",
       "  'eos_token_id': None,\n",
       "  'encoder_no_repeat_ngram_size': 0,\n",
       "  'decoder_start_token_id': None,\n",
       "  'is_assistant': False,\n",
       "  'num_assistant_tokens': 20,\n",
       "  'num_assistant_tokens_schedule': 'constant',\n",
       "  'assistant_confidence_threshold': 0.4,\n",
       "  'prompt_lookup_num_tokens': None,\n",
       "  'max_matching_ngram_size': None,\n",
       "  'generation_kwargs': {},\n",
       "  '_from_model_config': False,\n",
       "  'transformers_version': '4.46.3',\n",
       "  'return_full_text': False},\n",
       " 'explore': {'max_length': 20,\n",
       "  'max_new_tokens': 50,\n",
       "  'min_length': 0,\n",
       "  'min_new_tokens': None,\n",
       "  'early_stopping': False,\n",
       "  'max_time': None,\n",
       "  'stop_strings': None,\n",
       "  'do_sample': True,\n",
       "  'num_beams': np.int64(2),\n",
       "  'num_beam_groups': 1,\n",
       "  'penalty_alpha': None,\n",
       "  'dola_layers': None,\n",
       "  'use_cache': False,\n",
       "  'cache_implementation': None,\n",
       "  'cache_config': None,\n",
       "  'return_legacy_cache': None,\n",
       "  'temperature': array([0. , 0.3, 0.6, 0.9, 1.2, 1.5, 1.8]),\n",
       "  'top_k': array([ 0., 30., 60., 90.]),\n",
       "  'top_p': 1.0,\n",
       "  'min_p': None,\n",
       "  'typical_p': 1.0,\n",
       "  'epsilon_cutoff': 0.0,\n",
       "  'eta_cutoff': 0.0,\n",
       "  'diversity_penalty': 0.0,\n",
       "  'repetition_penalty': 1.0,\n",
       "  'encoder_repetition_penalty': 1.0,\n",
       "  'length_penalty': 1.0,\n",
       "  'no_repeat_ngram_size': 0,\n",
       "  'bad_words_ids': None,\n",
       "  'force_words_ids': None,\n",
       "  'renormalize_logits': False,\n",
       "  'constraints': None,\n",
       "  'forced_bos_token_id': None,\n",
       "  'forced_eos_token_id': None,\n",
       "  'remove_invalid_values': False,\n",
       "  'exponential_decay_length_penalty': None,\n",
       "  'suppress_tokens': None,\n",
       "  'begin_suppress_tokens': None,\n",
       "  'forced_decoder_ids': None,\n",
       "  'sequence_bias': None,\n",
       "  'token_healing': False,\n",
       "  'guidance_scale': None,\n",
       "  'low_memory': None,\n",
       "  'watermarking_config': None,\n",
       "  'num_return_sequences': 1,\n",
       "  'output_attentions': False,\n",
       "  'output_hidden_states': False,\n",
       "  'output_scores': False,\n",
       "  'output_logits': None,\n",
       "  'return_dict_in_generate': False,\n",
       "  'pad_token_id': None,\n",
       "  'bos_token_id': None,\n",
       "  'eos_token_id': None,\n",
       "  'encoder_no_repeat_ngram_size': 0,\n",
       "  'decoder_start_token_id': None,\n",
       "  'is_assistant': False,\n",
       "  'num_assistant_tokens': 20,\n",
       "  'num_assistant_tokens_schedule': 'constant',\n",
       "  'assistant_confidence_threshold': 0.4,\n",
       "  'prompt_lookup_num_tokens': None,\n",
       "  'max_matching_ngram_size': None,\n",
       "  'generation_kwargs': {},\n",
       "  '_from_model_config': False,\n",
       "  'transformers_version': '4.46.3',\n",
       "  'return_full_text': False}}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%writefile enforce_decode_params.py \n",
    "\n",
    "\"\"\" Generating synthetic dataset for testing Decoding Strategies. \"\"\"\n",
    "\n",
    "import numpy as np \n",
    "from typing import Literal \n",
    "from dataclasses import dataclass\n",
    "\n",
    "SEED = 42\n",
    "class Meter:\n",
    "\t\"\"\" Default Meter holding ranges / vals common and shared by all decoding strategies. \"\"\"\n",
    " \n",
    "\tT = 30 # sampling param / spacer x_{i-1} - x_i\n",
    "\tmin_k = 0.0 \n",
    "\tmax_k = 100\n",
    "\tmin_p = 0.0 \n",
    "\tmax_p = 1.0 \n",
    "\trepitition_penalty_min = 1 \n",
    "\trepitition_penalty_max = 2\n",
    "\ttemperature_min = 0.0 \n",
    "\ttemperature_max = 2.0 \n",
    "\talpha_min = 0.0 \n",
    "\talpha_max = 1.0 \n",
    "\tbeam_width_min = 2\n",
    "\tbeam_width_max = 5 # inf\n",
    "\t# Default Attributes\n",
    "\tmax_new_tokens=50\n",
    "\tnum_return_sequences=1\n",
    "\tnum_beams=1 # default \n",
    "\tnum_beam_groups=1 # default groups \n",
    "\n",
    "GEN_ARGS = GenerationConfig(\n",
    "\tdo_sample=True,\n",
    "\tuse_cache=False, \n",
    "\treturn_full_text=False, # False to only return the model's output text and not the users inputs \n",
    "\tmax_new_tokens=Meter.max_new_tokens,\n",
    ")\n",
    "\n",
    "class Sampler:\n",
    "\tGreedySearch = GenerationConfig(\n",
    "\t\tdo_sample=False, \n",
    "\t\tuse_cache=False, \n",
    "\t\treturn_full_text=False,\n",
    "\t\tmax_new_tokens=Meter.max_new_tokens,\n",
    "\t\tnum_return_sequences=Meter.num_return_sequences, \n",
    "\t)\n",
    "\t\n",
    "\t@dataclass \n",
    "\tclass BeamSearch:\n",
    "\t\ttop_k = np.arange(Meter.min_k, stop=Meter.max_k, step=Meter.T) # [ 0, 15, 30, 45, 60, 75, 90]\n",
    "\t\ttemperature = np.arange(Meter.temperature_min, Meter.temperature_max, step=Meter.T/100)\n",
    "\t\tnum_beams = np.arange(Meter.beam_width_min, stop=Meter.beam_width_max, step=Meter.T) # or beam_widths = search area size per leaf step or n.o of branches to retain at each step\n",
    "  \n",
    "\t@dataclass \n",
    "\tclass DiverseBeamSearch:\n",
    "\t\tnum_beam_groups = 5\n",
    "\n",
    "\t@dataclass \n",
    "\tclass NucleusSearch:\n",
    "\t\ttop_p = np.arange(Meter.min_p, stop=Meter.max_p, step=Meter.T) # [0.  , 0.15, 0.3 , 0.45, 0.6 , 0.75, 0.9 ]\n",
    "\t\ttemperature = np.arange(Meter.temperature_min, Meter.temperature_max, step=Meter.T/100)\n",
    "\n",
    "\t@dataclass \n",
    "\tclass ContrastiveSearch(NucleusSearch):\n",
    "\t\tpenalty_alpha = np.arange(Meter.alpha_min, Meter.alpha_max, Meter.T / 100)\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef exploreExploit(explore_type: Literal['Contrastive', 'BeamSearch', 'LogitScaler', 'NucleusSearch'] = 'BeamSearch'):\n",
    "\t\t\"\"\" Creates Exploiting and Exploring Feature Space.  \"\"\"\n",
    "  \t\t\n",
    "    \t# Get the parameters from each class\n",
    "\t\tgreedy = Sampler.GreedySearch.to_dict()\n",
    "\t\texplore = GEN_ARGS.to_dict().copy()\n",
    "  \n",
    "\t\tif explore_type == 'BeamSearch':\n",
    "\t\t\tattr = {'top_k': Sampler.BeamSearch.top_k, 'temperature': Sampler.BeamSearch.temperature, 'num_beams': Sampler.BeamSearch.num_beams[0]}\n",
    "\t\telif explore_type == 'NucleusSearch':\n",
    "\t\t\tattr = {'top_p': Sampler.NucleusSearch.top_p, 'temperature': Sampler.NucleusSearch.temperature}\n",
    "\t\telif explore_type == 'Contrastive':\n",
    "\t\t\tattr = {'top_p': Sampler.ContrastiveSearch.top_p, 'temperature': Sampler.ContrastiveSearch.temperature, 'penalty_alpha': Sampler.ContrastiveSearch.penalty_alpha}\n",
    "\t\telse:\n",
    "\t\t\traise ValueError('Retry pls. Unacceptable Decoding Strategy')\n",
    "\t\t\n",
    "\t\texplore.update(attr)\n",
    "\t\treturn {'greedy': greedy, 'explore': explore}\n",
    "\n",
    "def preprocess(pipe, user_inputs):\n",
    "\treturn pipe.tokenizer.apply_chat_template(\n",
    "\t\tuser_inputs, \n",
    "\t\ttokenize=False, \n",
    "\t\tadd_special_tokens=False\n",
    "\t)\n",
    "\t\n",
    "def chatbot(pipe, chat_inputs, **kwargs):\n",
    "\t\"\"\" Function contacting with the GPT Pipeline. \"\"\"\n",
    "\tgen_args = GEN_ARGS.to_dict()\n",
    "\tif kwargs:\n",
    "\t\tgen_args.update(kwargs)\n",
    "\t\n",
    "\tresponse = pipe(chat_inputs, **gen_args)\n",
    " \n",
    "\tif not isinstance(response, list):\n",
    "\t\traise ValueError(f\"Expected list but got {response}\")\n",
    "\tif not response[0].get('generated_text', None):\n",
    "\t\traise ValueError(f\"Expected 'generated_text' key in response but got {response}\")\n",
    "\t\n",
    "\tprint('Response:\\n', response[0]['generated_text'])\n",
    "\treturn response[0]['generated_text']\n",
    "\n",
    "Sampler.exploreExploit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Both `max_new_tokens` (=50) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      " Oh my gosh it's you!!! It even is, just a reminder this was your birthday yesterday. Happy Easter lt 3 I'm a simple man.\n"
     ]
    }
   ],
   "source": [
    "chat_input = generator.tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {'role': 'system', 'content': 'Your name is Lucifer, chatting with a stranger online.'},\n",
    "\t\t{'role': 'user', 'content': 'hey what is your name? :)'},\n",
    "\t\t{'role': 'assistant', 'content': 'My name is Lucifer.'}, \n",
    "\t\t{'role': 'user', 'content': 'How are you doing today?'}\n",
    "\t], \n",
    "    tokenize=False, \n",
    "    add_special_tokens=False\n",
    ")\n",
    "\n",
    "response = generator(chat_input, **GEN_ARGS.to_dict())\n",
    "\n",
    "if not isinstance(response, list):\n",
    "    raise ValueError(f\"Expected list but got {response}\")\n",
    "if not response[0].get('generated_text', None):\n",
    "    raise ValueError(f\"Expected 'generated_text' key in response but got {response}\")\n",
    "\n",
    "print('Response:\\n', response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objects type name: BeamSearch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      " Hi and welcome!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      " Hello! How are you today?\n",
      "Response:\n",
      " Hey you\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      " Hi and welcome!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      " I'm always there in spirit! Welcome to the sub!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      " OOC : Welcome c : What's been keeping u ceegojk from writing her usual speech after he was done hmu with his name so it could be a nice place...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      " Hi and welcome!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      " Oo I think I did it\n",
      "Response:\n",
      " What has your friends ever done these days because not caring so very much is generally enough for me haha! lt 3\n"
     ]
    }
   ],
   "source": [
    "#%%writefile agent_dial_utils.py\n",
    "\n",
    "\"\"\" \n",
    "\n",
    "\tagent_dial_utils.py \n",
    "\t\n",
    "\tScript contains:\n",
    "\t- Utils for creating experiment's decoder params combinations and dataclasses \n",
    "\t- Data handlers and Properties for Different Decoding strategies\n",
    " \n",
    "\"\"\"\n",
    "\n",
    "from enum import Enum \n",
    "from itertools import product \n",
    "from dataclasses import dataclass, field \n",
    "\n",
    "class SearchConfig(Enum):\n",
    "\tBeamSearch = (\n",
    "\t\t('top_k', 20, 100), \n",
    "\t\t('temperature', 0.1, 2.0), \n",
    "\t\t('num_beams', 1, 3)\n",
    "\t)\n",
    "\n",
    "# aligns with the normal params for medium - neutral state\n",
    "class DefaultParams(Enum):\n",
    "\ttop_k = 50\n",
    "\ttemperature = 1.0 \n",
    "\tnum_beams = 1\n",
    "\t\n",
    "@dataclass \n",
    "class BeamSearch:\n",
    "\ttop_k: float = field(default=DefaultParams.top_k.value)\n",
    "\ttemperature: float = field(default=DefaultParams.temperature.value)\n",
    "\tnum_beams: int = field(default=1)\n",
    "\t\n",
    "\t@classmethod\n",
    "\tdef packet(cls, top_k, temperature):\n",
    "\t\treturn product(top_k, temperature, repeat=1)\n",
    "\n",
    "def sampleParams(object, delta_t=3):\n",
    "\tdecode_method = object.__class__.__qualname__ if object.__class__.__qualname__ != 'type' else object.__qualname__\n",
    "\tprint('Objects type name:', decode_method) \n",
    "\t# Find the corresponding configuration in SearchConfig Enum\n",
    "\tconfig = getattr(SearchConfig, decode_method, None)\n",
    "\tif config is None:\n",
    "\t\traise ValueError(f\"Unknown search method: {decode_method}\")\n",
    "\t\n",
    "\t# Loop through the tuple (parameter name, min value, max value) and sample using np.linspace\n",
    "\tsamples = {}\n",
    "\tfor param, min_val, max_val in config.value:\n",
    "\t\tsamples[param] = np.linspace(min_val, max_val, delta_t, dtype=type(min_val))\n",
    "\n",
    "\treturn samples\n",
    "\n",
    "beam_search = BeamSearch(0.9, 0.5, 1)\n",
    "params = sampleParams(beam_search)\n",
    "param_comb = list(BeamSearch.packet(params['top_k'], params['temperature']))\n",
    "\n",
    "user_input = [{'role': 'user', 'content': 'Hey how are you today! :)'}]\n",
    "chat_input = preprocess(generator, user_input)\n",
    "\n",
    "result = []\n",
    "for top_k_, temp_ in param_comb: \n",
    "\targs = dict(top_k=top_k_.item(), temperature=temp_.item(), max_length=None)\n",
    "\toutput = chatbot(pipe=generator, chat_inputs=user_input, **args)\n",
    "\tresult.append(\n",
    "\t\t{\n",
    "\t\t\t'output': output, \n",
    "\t\t\t'params': args, \n",
    "\t\t\t'method': 'beam_search'\n",
    "\t\t}\n",
    "\t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>output</th>\n",
       "      <th>params</th>\n",
       "      <th>method</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi and welcome!</td>\n",
       "      <td>{'top_k': 20, 'temperature': 0.1, 'max_length'...</td>\n",
       "      <td>beam_search</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hello! How are you today?</td>\n",
       "      <td>{'top_k': 20, 'temperature': 1.05, 'max_length...</td>\n",
       "      <td>beam_search</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey you</td>\n",
       "      <td>{'top_k': 20, 'temperature': 2.0, 'max_length'...</td>\n",
       "      <td>beam_search</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hi and welcome!</td>\n",
       "      <td>{'top_k': 60, 'temperature': 0.1, 'max_length'...</td>\n",
       "      <td>beam_search</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm always there in spirit! Welcome to the sub!</td>\n",
       "      <td>{'top_k': 60, 'temperature': 1.05, 'max_length...</td>\n",
       "      <td>beam_search</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>OOC : Welcome c : What's been keeping u ceegoj...</td>\n",
       "      <td>{'top_k': 60, 'temperature': 2.0, 'max_length'...</td>\n",
       "      <td>beam_search</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Hi and welcome!</td>\n",
       "      <td>{'top_k': 100, 'temperature': 0.1, 'max_length...</td>\n",
       "      <td>beam_search</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Oo I think I did it</td>\n",
       "      <td>{'top_k': 100, 'temperature': 1.05, 'max_lengt...</td>\n",
       "      <td>beam_search</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What has your friends ever done these days bec...</td>\n",
       "      <td>{'top_k': 100, 'temperature': 2.0, 'max_length...</td>\n",
       "      <td>beam_search</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              output  \\\n",
       "0                                    Hi and welcome!   \n",
       "1                          Hello! How are you today?   \n",
       "2                                            Hey you   \n",
       "3                                    Hi and welcome!   \n",
       "4    I'm always there in spirit! Welcome to the sub!   \n",
       "5  OOC : Welcome c : What's been keeping u ceegoj...   \n",
       "6                                    Hi and welcome!   \n",
       "7                                Oo I think I did it   \n",
       "8  What has your friends ever done these days bec...   \n",
       "\n",
       "                                              params       method  \n",
       "0  {'top_k': 20, 'temperature': 0.1, 'max_length'...  beam_search  \n",
       "1  {'top_k': 20, 'temperature': 1.05, 'max_length...  beam_search  \n",
       "2  {'top_k': 20, 'temperature': 2.0, 'max_length'...  beam_search  \n",
       "3  {'top_k': 60, 'temperature': 0.1, 'max_length'...  beam_search  \n",
       "4  {'top_k': 60, 'temperature': 1.05, 'max_length...  beam_search  \n",
       "5  {'top_k': 60, 'temperature': 2.0, 'max_length'...  beam_search  \n",
       "6  {'top_k': 100, 'temperature': 0.1, 'max_length...  beam_search  \n",
       "7  {'top_k': 100, 'temperature': 1.05, 'max_lengt...  beam_search  \n",
       "8  {'top_k': 100, 'temperature': 2.0, 'max_length...  beam_search  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(result)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "\n",
    "\tCreates toy dataset to play with\n",
    "\n",
    "\"\"\"\n",
    "from datasets import load_dataset\n",
    "from dataclasses import dataclass\n",
    "\n",
    "tag_map = {\n",
    "\t\"action\": [\"unknown\", \"inform\", \"question\", \"directive\", \"commissive\"],\n",
    "\t\"emotion\": [\"neutral\", \"anger\", \"disgust\", \"fear\", \"happiness\", \"sadness\", \"surprise\"]\n",
    "}\n",
    "emotion_map = {\n",
    "\t'happiness': 'positive', 'surprise': 'positive',\n",
    "\t'anger': 'negative', 'disgust': 'negative',\n",
    "\t'fear': 'negative', 'sadness': 'negative'\n",
    "}\n",
    "\n",
    "@dataclass\n",
    "class Groupon:\n",
    "\tcandidate_labels: list \n",
    "\t\n",
    "\t@property \n",
    "\tdef N(self):\n",
    "\t\treturn len(self.candidate_labels)\n",
    "\n",
    "\t@property\n",
    "\tdef id2label(self):\n",
    "\t\treturn dict(zip(range(self.N), self.candidate_labels))\n",
    "\n",
    "\t@property\n",
    "\tdef label2id(self):\n",
    "\t\treturn dict(zip(self.candidate_labels, range(self.N)))\n",
    "\n",
    "\tdef label_input(self, item):\n",
    "\t\tif isinstance(item, str): \n",
    "\t\t\treturn self.label2id[item]\n",
    "\t\telif isinstance(item, int): \n",
    "\t\t\treturn self.id2label[item]\n",
    "\n",
    "def process_data(row):\n",
    "\t\"\"\"\n",
    "\tProcess the row by splitting the dialog into 'user' and 'assistant' messages \n",
    "\tand assigning corresponding emotions to 'u_big_emote' and 'a_big_emote'.\n",
    "\t\"\"\"\n",
    "\t# Initialize columns\n",
    "\trow['user'] = []\n",
    "\trow['assistant'] = []\n",
    "\trow['u_act'] = []\n",
    "\trow['a_act'] = []\n",
    "\trow['u_big_emote'] = []\n",
    "\trow['a_big_emote'] = []\n",
    "\t\n",
    "\n",
    "\t# Process the dialog and emotions\n",
    "\tfor idx in range(0, len(row['dialog']) - 1, 2):\n",
    "\t\trow['user'].append(row['dialog'][idx])\n",
    "\t\trow['assistant'].append(row['dialog'][idx + 1])\n",
    "\n",
    "\n",
    "\t\trow['u_big_emote'].append(emotion_map.get(emoticon.label_input(row['emotion'][idx]), 'neutral'))\n",
    "\t\trow['a_big_emote'].append(emotion_map.get(emoticon.label_input(row['emotion'][idx + 1]), 'neutral'))\n",
    "\t\trow['u_act'].append(action.label_input(row['act'][idx]))\n",
    "\t\trow['a_act'].append(action.label_input(row['act'][idx+1]))\n",
    "  \n",
    "\treturn row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>assistant</th>\n",
       "      <th>u_act</th>\n",
       "      <th>a_act</th>\n",
       "      <th>u_big_emote</th>\n",
       "      <th>a_big_emote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Say , Jim , how about going for a few beers a...</td>\n",
       "      <td>[ You know that is tempting but is really not ...</td>\n",
       "      <td>[directive, question, question, commissive, di...</td>\n",
       "      <td>[commissive, question, directive, inform, comm...</td>\n",
       "      <td>[neutral, neutral, neutral, positive, positive]</td>\n",
       "      <td>[neutral, neutral, neutral, positive, positive]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Can you do push-ups ? ,  Really ? I think tha...</td>\n",
       "      <td>[ Of course I can . It's a piece of cake ! Bel...</td>\n",
       "      <td>[question, question, inform]</td>\n",
       "      <td>[inform, question, inform]</td>\n",
       "      <td>[neutral, positive, neutral]</td>\n",
       "      <td>[neutral, neutral, neutral]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Can you study with the radio on ? ,  What is ...</td>\n",
       "      <td>[ No , I listen to background music . ,  The r...</td>\n",
       "      <td>[question, question]</td>\n",
       "      <td>[inform, inform]</td>\n",
       "      <td>[neutral, neutral]</td>\n",
       "      <td>[neutral, neutral]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Are you all right ? ,  Don't worry.He is an a...</td>\n",
       "      <td>[ I will be all right soon . I was terrified w...</td>\n",
       "      <td>[question, inform]</td>\n",
       "      <td>[inform, inform]</td>\n",
       "      <td>[neutral, neutral]</td>\n",
       "      <td>[neutral, neutral]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Hey John , nice skates . Are they new ? ,  Wh...</td>\n",
       "      <td>[ Yeah , I just got them . I started playing i...</td>\n",
       "      <td>[question, question, inform, inform]</td>\n",
       "      <td>[inform, inform, question, directive]</td>\n",
       "      <td>[neutral, neutral, neutral, neutral]</td>\n",
       "      <td>[neutral, neutral, positive, positive]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>[How was your education going on in Australia ...</td>\n",
       "      <td>[ I'm going to graduate this summer . ,  I'm p...</td>\n",
       "      <td>[question, question, directive]</td>\n",
       "      <td>[inform, inform, commissive]</td>\n",
       "      <td>[neutral, neutral, neutral]</td>\n",
       "      <td>[neutral, neutral, neutral]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>[Do you have any particular hobbies , Tom ? , ...</td>\n",
       "      <td>[ Oh , yes . I love playing badminton , table ...</td>\n",
       "      <td>[question, question, question, inform]</td>\n",
       "      <td>[inform, inform, inform, inform]</td>\n",
       "      <td>[neutral, neutral, neutral, neutral]</td>\n",
       "      <td>[neutral, neutral, neutral, neutral]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>[What ’ s the plot of your new movie ? ,  Did ...</td>\n",
       "      <td>[ It ’ s a story about a policemen who is inve...</td>\n",
       "      <td>[question, question, question, question, inform]</td>\n",
       "      <td>[inform, inform, inform, inform, question]</td>\n",
       "      <td>[neutral, neutral, neutral, neutral, positive]</td>\n",
       "      <td>[neutral, neutral, neutral, neutral, positive]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>[Who's that old lady trimming the trees ? ,  S...</td>\n",
       "      <td>[ She's my grandma . ,  92 . ]</td>\n",
       "      <td>[question, question]</td>\n",
       "      <td>[inform, inform]</td>\n",
       "      <td>[neutral, neutral]</td>\n",
       "      <td>[neutral, neutral]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>[Mom . My legs are killing me . ]</td>\n",
       "      <td>[ Hold on . We will be successful right away . ]</td>\n",
       "      <td>[directive]</td>\n",
       "      <td>[commissive]</td>\n",
       "      <td>[neutral]</td>\n",
       "      <td>[neutral]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 user  \\\n",
       "0   [Say , Jim , how about going for a few beers a...   \n",
       "1   [Can you do push-ups ? ,  Really ? I think tha...   \n",
       "2   [Can you study with the radio on ? ,  What is ...   \n",
       "3   [Are you all right ? ,  Don't worry.He is an a...   \n",
       "4   [Hey John , nice skates . Are they new ? ,  Wh...   \n",
       "..                                                ...   \n",
       "95  [How was your education going on in Australia ...   \n",
       "96  [Do you have any particular hobbies , Tom ? , ...   \n",
       "97  [What ’ s the plot of your new movie ? ,  Did ...   \n",
       "98  [Who's that old lady trimming the trees ? ,  S...   \n",
       "99                  [Mom . My legs are killing me . ]   \n",
       "\n",
       "                                            assistant  \\\n",
       "0   [ You know that is tempting but is really not ...   \n",
       "1   [ Of course I can . It's a piece of cake ! Bel...   \n",
       "2   [ No , I listen to background music . ,  The r...   \n",
       "3   [ I will be all right soon . I was terrified w...   \n",
       "4   [ Yeah , I just got them . I started playing i...   \n",
       "..                                                ...   \n",
       "95  [ I'm going to graduate this summer . ,  I'm p...   \n",
       "96  [ Oh , yes . I love playing badminton , table ...   \n",
       "97  [ It ’ s a story about a policemen who is inve...   \n",
       "98                     [ She's my grandma . ,  92 . ]   \n",
       "99   [ Hold on . We will be successful right away . ]   \n",
       "\n",
       "                                                u_act  \\\n",
       "0   [directive, question, question, commissive, di...   \n",
       "1                        [question, question, inform]   \n",
       "2                                [question, question]   \n",
       "3                                  [question, inform]   \n",
       "4                [question, question, inform, inform]   \n",
       "..                                                ...   \n",
       "95                    [question, question, directive]   \n",
       "96             [question, question, question, inform]   \n",
       "97   [question, question, question, question, inform]   \n",
       "98                               [question, question]   \n",
       "99                                        [directive]   \n",
       "\n",
       "                                                a_act  \\\n",
       "0   [commissive, question, directive, inform, comm...   \n",
       "1                          [inform, question, inform]   \n",
       "2                                    [inform, inform]   \n",
       "3                                    [inform, inform]   \n",
       "4               [inform, inform, question, directive]   \n",
       "..                                                ...   \n",
       "95                       [inform, inform, commissive]   \n",
       "96                   [inform, inform, inform, inform]   \n",
       "97         [inform, inform, inform, inform, question]   \n",
       "98                                   [inform, inform]   \n",
       "99                                       [commissive]   \n",
       "\n",
       "                                        u_big_emote  \\\n",
       "0   [neutral, neutral, neutral, positive, positive]   \n",
       "1                      [neutral, positive, neutral]   \n",
       "2                                [neutral, neutral]   \n",
       "3                                [neutral, neutral]   \n",
       "4              [neutral, neutral, neutral, neutral]   \n",
       "..                                              ...   \n",
       "95                      [neutral, neutral, neutral]   \n",
       "96             [neutral, neutral, neutral, neutral]   \n",
       "97   [neutral, neutral, neutral, neutral, positive]   \n",
       "98                               [neutral, neutral]   \n",
       "99                                        [neutral]   \n",
       "\n",
       "                                        a_big_emote  \n",
       "0   [neutral, neutral, neutral, positive, positive]  \n",
       "1                       [neutral, neutral, neutral]  \n",
       "2                                [neutral, neutral]  \n",
       "3                                [neutral, neutral]  \n",
       "4            [neutral, neutral, positive, positive]  \n",
       "..                                              ...  \n",
       "95                      [neutral, neutral, neutral]  \n",
       "96             [neutral, neutral, neutral, neutral]  \n",
       "97   [neutral, neutral, neutral, neutral, positive]  \n",
       "98                               [neutral, neutral]  \n",
       "99                                        [neutral]  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#action = Groupon(tag_map['action'])\n",
    "#emoticon = Groupon(tag_map['emotion'])\n",
    "\n",
    "#ds = load_dataset('li2017dailydialog/daily_dialog', split=\"train[:100]\", trust_remote_code=True)\n",
    "#ds = ds.map(process_data, remove_columns=['dialog', 'act', 'emotion'])\n",
    "# ds.to_csv('sample_beam_search.csv', index=False)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding data batch 79\n",
      "Adding data batch 37\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\"\"\" \n",
    "\tgpt_decode_params.py\n",
    "\t---------------------\n",
    "\tExperiment Runners / Helpers for defining optimal decoding Parameters for the GPT-2 Model relative to the following features:\n",
    " \t- Current human speaker's features in observation window (e.g. chat window):\n",
    "  \t\t- Emotions\n",
    "\t\t- Engagement / Interests \n",
    "\t- Agent's expected features (built from main i.e. myself):\n",
    "\t\t- Emotions \n",
    "\t\t- Engagement / Interests \n",
    "\t- Current Window combining Agent's and speaker's chat dynamics: \n",
    "\t\t- Synchronicity of engagement \n",
    " \n",
    "\tObjectives:\n",
    "\t- Define the optimal decoding parameters for the GPT-2 model based on the current speaker's features in the observation window.\n",
    "\t- Define the optimal decoding parameters for the GPT-2 model based on the agent's expected features.\n",
    "\t- Define the optimal decoding parameters for the GPT-2 model based on the current window combining the agent's and speaker's chat dynamics.\n",
    "\t- Preprocessors / Scalers for Human Features. \n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "time_window = 5 # 5 pairs of utterances \n",
    "FEATURES = {\n",
    "\t'emotion': ['positive', 'negative', 'neutral'], \n",
    "\t'action': action, \n",
    "}\n",
    "\n",
    "# caching handlers \n",
    "session = []\n",
    "\n",
    "def build_history_space(session):\n",
    "\t\"\"\" Construct Priori's or initial state params for each feature space.  \"\"\"\n",
    "\tpass \n",
    "\n",
    "def session_listener(func):\n",
    "\tdef wrapper(*args, **kwargs):\n",
    "\t\tif len(func.featMetrics) > time_window:\n",
    "\t\t\tsession.append(func.featMetrics)\n",
    "\t\t\tbuild_history_space(session)\n",
    "\t\t\tfunc.featMetrics = [] \n",
    "\t\treturn func(*args, **kwargs)\n",
    "\tprint(f'Session Caches: {len(session)}')\n",
    "\treturn wrapper\n",
    "\n",
    "class ChatSession:\n",
    "\t\"\"\" \n",
    " \tMaps responses to the model outputs. \n",
    " \tObjective: A function that maps the engagement of the user. \n",
    "   \t\"\"\"\n",
    "\t\n",
    "\tdef __init__(self):\n",
    "\t\tself.history = []\n",
    "\n",
    "\t@property \n",
    "\tdef agent_history(self):\n",
    "\t\tdiary = pd.DataFrame(self.history)\n",
    "\t\thistory = diary[diary['role'] == 'assistant']['content'].values\n",
    "\t\treturn history \n",
    "\n",
    "\t@property \n",
    "\tdef user_history(self): \n",
    "\t\tdiary = pd.DataFrame(self.history)\n",
    "\t\thistory = diary[diary['role'] == 'user']['content'].values \n",
    "\t\treturn history \n",
    "\n",
    "\tdef update_history(self, user_input, agent_input):\n",
    "\t\t\"\"\" Updates Pair of Speech Utterances. \"\"\"\n",
    "\t\tself.history.append({\n",
    "\t\t\t'role': 'user', \n",
    "\t\t\t'content': user_input \n",
    "\t\t})\n",
    "\t\tself.history.append({\n",
    "\t\t\t'role': 'assistant', \n",
    "\t\t\t'content': agent_input\n",
    "\t\t})\n",
    "  \n",
    "def run_episodes(num_episodes=2):\n",
    "\t# 2 chat dialogues ==> 2 strangers \n",
    "\tchat = ChatSession()\n",
    "\ty_true = []\n",
    " \n",
    "\tfor row_idx, row in ds.to_pandas().sample(num_episodes).iterrows():\n",
    "\t\tprint(f'Adding data batch {row_idx}')\n",
    "\t\tfor user_input, agent_input in zip(row['user'], row['assistant']):\n",
    "\t\t\tchat.update_history(user_input, agent_input)\n",
    "\n",
    "\t\ty_true.append({\n",
    "\t\t\t'u_act': row['u_act'], \n",
    "\t\t\t'a_act': row['a_act'], \n",
    "\t\t\t'u_big_emote': row['u_big_emote'], \n",
    "\t\t\t'a_big_emote': row['a_big_emote']\n",
    "\t\t})\n",
    "\n",
    "\treturn chat, y_true\n",
    "\n",
    "# creates mock data \n",
    "chat, y_true = run_episodes(FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "# creates inferencing chain \n",
    "\n",
    "class OnlineChain:\n",
    "\tdef __init__(self, features, model_card: str = \"facebook/bart-large-mnli\", task_label: str = \"zero-shot-classification\"):\n",
    "\t\tself.clf = pipeline(task_label, model_card)\n",
    "\t\tself.features = {}\n",
    "\t\tfor task_label, feat_label in features.items():\n",
    "\t\t\tfeat = Groupon(feat_label) if not isinstance(feat_label, Groupon) else feat_label \n",
    "\t\t\tself.features[task_label] = (self.classifier, feat)\n",
    "\n",
    "\tdef classifier(self, inputs, candidate_labels):\n",
    "\t\toutput = self.clf(inputs, candidate_labels)\n",
    "\t\tassert set(output.keys()) == set(['sequence', 'labels', 'scores']), f'Incorrect output from classifier: {output}'\n",
    "\t\treturn dict(zip(output['labels'], output['scores']))\n",
    "\n",
    "\tdef model_chain(self, inputs: str):\n",
    "\t\tresult = {}\n",
    "\t\tfor task, (model, groupon) in self.features.items():\n",
    "\t\t\tresult[task] = model(inputs=inputs, candidate_labels=groupon.candidate_labels)\n",
    "\t\treturn result \n",
    "\n",
    "chain = OnlineChain(features={'emotion': ['positive', 'negative', 'neutral']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Hi and welcome!', 'Hello! How are you today?', 'Hey you',\n",
       "       'Hi and welcome!',\n",
       "       \"I'm always there in spirit! Welcome to the sub!\",\n",
       "       \"OOC : Welcome c : What's been keeping u ceegojk from writing her usual speech after he was done hmu with his name so it could be a nice place...\",\n",
       "       'Hi and welcome!', 'Oo I think I did it',\n",
       "       'What has your friends ever done these days because not caring so very much is generally enough for me haha! lt 3'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_result = data['output'].values\n",
    "example_result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_output = {}\n",
    "for item in example_result: \n",
    "    chain_output[item] = chain.model_chain(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['inferenced_emotion'] = data['output'].map(chain_output)\n",
    "\n",
    "data['emote_label'] = data['inferenced_emotion'].apply(\n",
    "    lambda x: max(x['emotion'], key=x['emotion'].get)\n",
    ")\n",
    "data['emote_score'] = data['inferenced_emotion'].apply(\n",
    "    lambda x: max(x['emotion'].values())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>output</th>\n",
       "      <th>params</th>\n",
       "      <th>method</th>\n",
       "      <th>inferenced_emotion</th>\n",
       "      <th>emote_label</th>\n",
       "      <th>emote_score</th>\n",
       "      <th>top_k</th>\n",
       "      <th>temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi and welcome!</td>\n",
       "      <td>{'top_k': 20, 'temperature': 0.1, 'max_length'...</td>\n",
       "      <td>beam_search</td>\n",
       "      <td>{'emotion': {'positive': 0.8815580010414124, '...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.881558</td>\n",
       "      <td>20</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hello! How are you today?</td>\n",
       "      <td>{'top_k': 20, 'temperature': 1.05, 'max_length...</td>\n",
       "      <td>beam_search</td>\n",
       "      <td>{'emotion': {'positive': 0.6990548968315125, '...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.699055</td>\n",
       "      <td>20</td>\n",
       "      <td>1.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey you</td>\n",
       "      <td>{'top_k': 20, 'temperature': 2.0, 'max_length'...</td>\n",
       "      <td>beam_search</td>\n",
       "      <td>{'emotion': {'positive': 0.5986918807029724, '...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.598692</td>\n",
       "      <td>20</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hi and welcome!</td>\n",
       "      <td>{'top_k': 60, 'temperature': 0.1, 'max_length'...</td>\n",
       "      <td>beam_search</td>\n",
       "      <td>{'emotion': {'positive': 0.8815580010414124, '...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.881558</td>\n",
       "      <td>60</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm always there in spirit! Welcome to the sub!</td>\n",
       "      <td>{'top_k': 60, 'temperature': 1.05, 'max_length...</td>\n",
       "      <td>beam_search</td>\n",
       "      <td>{'emotion': {'positive': 0.7329865097999573, '...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.732987</td>\n",
       "      <td>60</td>\n",
       "      <td>1.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>OOC : Welcome c : What's been keeping u ceegoj...</td>\n",
       "      <td>{'top_k': 60, 'temperature': 2.0, 'max_length'...</td>\n",
       "      <td>beam_search</td>\n",
       "      <td>{'emotion': {'positive': 0.46241477131843567, ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.462415</td>\n",
       "      <td>60</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Hi and welcome!</td>\n",
       "      <td>{'top_k': 100, 'temperature': 0.1, 'max_length...</td>\n",
       "      <td>beam_search</td>\n",
       "      <td>{'emotion': {'positive': 0.8815580010414124, '...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.881558</td>\n",
       "      <td>100</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Oo I think I did it</td>\n",
       "      <td>{'top_k': 100, 'temperature': 1.05, 'max_lengt...</td>\n",
       "      <td>beam_search</td>\n",
       "      <td>{'emotion': {'positive': 0.8012709617614746, '...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.801271</td>\n",
       "      <td>100</td>\n",
       "      <td>1.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What has your friends ever done these days bec...</td>\n",
       "      <td>{'top_k': 100, 'temperature': 2.0, 'max_length...</td>\n",
       "      <td>beam_search</td>\n",
       "      <td>{'emotion': {'negative': 0.49085739254951477, ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.490857</td>\n",
       "      <td>100</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              output  \\\n",
       "0                                    Hi and welcome!   \n",
       "1                          Hello! How are you today?   \n",
       "2                                            Hey you   \n",
       "3                                    Hi and welcome!   \n",
       "4    I'm always there in spirit! Welcome to the sub!   \n",
       "5  OOC : Welcome c : What's been keeping u ceegoj...   \n",
       "6                                    Hi and welcome!   \n",
       "7                                Oo I think I did it   \n",
       "8  What has your friends ever done these days bec...   \n",
       "\n",
       "                                              params       method  \\\n",
       "0  {'top_k': 20, 'temperature': 0.1, 'max_length'...  beam_search   \n",
       "1  {'top_k': 20, 'temperature': 1.05, 'max_length...  beam_search   \n",
       "2  {'top_k': 20, 'temperature': 2.0, 'max_length'...  beam_search   \n",
       "3  {'top_k': 60, 'temperature': 0.1, 'max_length'...  beam_search   \n",
       "4  {'top_k': 60, 'temperature': 1.05, 'max_length...  beam_search   \n",
       "5  {'top_k': 60, 'temperature': 2.0, 'max_length'...  beam_search   \n",
       "6  {'top_k': 100, 'temperature': 0.1, 'max_length...  beam_search   \n",
       "7  {'top_k': 100, 'temperature': 1.05, 'max_lengt...  beam_search   \n",
       "8  {'top_k': 100, 'temperature': 2.0, 'max_length...  beam_search   \n",
       "\n",
       "                                  inferenced_emotion emote_label  emote_score  \\\n",
       "0  {'emotion': {'positive': 0.8815580010414124, '...    positive     0.881558   \n",
       "1  {'emotion': {'positive': 0.6990548968315125, '...    positive     0.699055   \n",
       "2  {'emotion': {'positive': 0.5986918807029724, '...    positive     0.598692   \n",
       "3  {'emotion': {'positive': 0.8815580010414124, '...    positive     0.881558   \n",
       "4  {'emotion': {'positive': 0.7329865097999573, '...    positive     0.732987   \n",
       "5  {'emotion': {'positive': 0.46241477131843567, ...    positive     0.462415   \n",
       "6  {'emotion': {'positive': 0.8815580010414124, '...    positive     0.881558   \n",
       "7  {'emotion': {'positive': 0.8012709617614746, '...    positive     0.801271   \n",
       "8  {'emotion': {'negative': 0.49085739254951477, ...    negative     0.490857   \n",
       "\n",
       "   top_k  temp  \n",
       "0     20  0.10  \n",
       "1     20  1.05  \n",
       "2     20  2.00  \n",
       "3     60  0.10  \n",
       "4     60  1.05  \n",
       "5     60  2.00  \n",
       "6    100  0.10  \n",
       "7    100  1.05  \n",
       "8    100  2.00  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['top_k'] = [i['top_k'] for i in data['params']]\n",
    "data['temp'] = [i['temperature'] for i in data['params']]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Drafts build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quick recap of problem** \n",
    "\n",
    "**Objective**\n",
    "The objective is to approximate the function between decoding parameters and dialogue engagement.\n",
    "\n",
    "The following script considers tackling this method with monte-carlo sampling methods. However, I think it is safer to do a bit more reading and perform statistical tests like t-tests etc. before going full in with this component.\n",
    "\n",
    "Predicting features I would like to target:\n",
    "To make this problem feasible, it is a good idea to constrain the predicting features to binary classes. \n",
    "\n",
    "- **Engagement Intensity** refers to the stable state representing the dynamics of speech exchange between the user and agent e.g. bored vs extremely interested.\n",
    "- **Emotion intensity** refers to responses that are minimal or absent in showing emotions (neutral) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "\t%%writefile draft_sampling.py \n",
    " \n",
    "\t- **Engagement** refers to the stable state representing the dynamics of speech exchange between the user and agent.\n",
    "\t- Note: The function won't be singular for the ObservationSpace, especially after maximizing dataset re-generation likelihood.\n",
    "\t- Formally, the objective is to identify a function that maps engagement levels to optimal decoding parameter combinations.\n",
    "\t- Desired properties of this function:\n",
    "\t- Bidirectional\n",
    "\t- Range: [-1, 1]\n",
    " \n",
    "\"\"\"\n",
    "\n",
    "#from sklearn.linear_model import BayesianRidge \n",
    "\n",
    "@dataclass\n",
    "class Gauss:\n",
    "\tmu: float = 0.0 \n",
    "\tstd: float = 1.0\n",
    "\n",
    "class KalmanFrame(object):\n",
    "\tdef __init__(self, state_space_dim, action_space_dim, time_window: int = 5):\n",
    "\t\tself.n = state_space_dim \n",
    "\t\tself.k = action_space_dim \n",
    "\t\tself.t = time_window # e.g. state space at 0 will be the initial state observed, here, time_window (t) = 0 would be state space observed in the first 5 seconds/logs (default to 5)\n",
    "  \n",
    "\tdef gain(self):\n",
    "\t\tpass \n",
    "\t\n",
    "\tdef prior(self):\n",
    "\t\tpass \n",
    "\t\n",
    "\tdef posterior(self):\n",
    "\t\tpass \n",
    "\n",
    "\n",
    "class CurveFittingRoom:\n",
    "    \"\"\" Workflow for inferencing tasks that deals with spaced time frames or where one of the controlling variables is over time (t). E.g. BayesRidgeRegression, Poly + LSQ\"\"\"\n",
    "    pass \n",
    "\n",
    "class ObservationSpace:\n",
    "\t\"\"\" Start this at the beginning of the chat window. \"\"\"\n",
    "\t\n",
    "\tdef __init__(self):\n",
    "\t \n",
    "\t\t# user and agent engagement level `[positive, negative, neutral]`. (i) Encoding to map the classification labels would make this a classification problem. (ii) Not encode the texts but focus specifically at the dialogue by interpreting as a frequency or resulting metrics from time-series transformation methods e.g. poly fitting / trig transformation \n",
    "  \n",
    "\t\tself.user_engagement = Gauss()\n",
    "\t\tself.agent_engagement = Gauss()\n",
    "  \n",
    "\t\t# dialogue's engagement engagement level considering both user and agent (stacks of 5 i.e. [[A_tx_t - b_k], [A_tx_t - b_k], ...]) where k < t (or k = t-1) mean(axis=0) == float\n",
    "\t\tself.observation = CurveFittingRoom() # func to approximate the dialogue's transitioning window from previous to current engagement `units`\n",
    "\t\n",
    "\t\n",
    "\tdef update(self, **kwargs):\n",
    "\t\tpass \n",
    "class Bandit:\n",
    "\t\"\"\" \n",
    "\t\tApproximates a function between the decoding params (representing GPT's emotional/engagement control for now)\n",
    "\t\tConsideration's to mind:\n",
    "\t\t\t- One output per decoding combination of parameters may not be accurate as unique texts are generated per run, Monte Carlo may need to be used here or Posterior / Prior estimations can be considered here. \n",
    "\n",
    "\t\t\t\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tdef __init__(self, decode_method: str = 'BeamSearch'):\n",
    "\t\tself.decode_method = decode_method \n",
    "\t\tself.params = getattr(SearchConfig, decode_method, None) \n",
    "\t\tif not self.params: \n",
    "\t\t\traise ValueError(f'ERror retrieving decoding method config: {decode_method}' )\n",
    "\t\t\n",
    "\t\tself.top_k = \n",
    "\t\tself.temperature = \n",
    "\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n",
      "\n",
      "user           [What do you do in your free time , Nancy ? , ...\n",
      "assistant      [ Well , I like playing the violin . ,  About ...\n",
      "u_act                       [question, question, inform, inform]\n",
      "a_act                         [inform, inform, question, inform]\n",
      "u_big_emote               [neutral, neutral, positive, positive]\n",
      "a_big_emote               [neutral, neutral, positive, positive]\n",
      "Name: 58, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# This is the main class in controlling generative's models texts \n",
    "\n",
    "class AgentDial:\n",
    "    def __init__(self, base_llm: pipeline, inference_chain: OnlineChain, decoder_params: dict):\n",
    "        self.llm = base_llm # gpt\n",
    "        self.inference_chain = inference_chain # returns the user features ... not the same as the chatting / dialogue helper agents chian \n",
    "        self.decoder_config = decoder_params \n",
    "\n",
    "    @property \n",
    "    def description(self):\n",
    "        \"\"\"\n",
    "        Returns a detailed description of the agent's decoder configuration, outlining the decoder strategy \n",
    "        and the associated parameter values to be explored during decoding. \n",
    "\n",
    "        The decoder configuration consists of keys representing the decoder strategies, each paired with a \n",
    "        list of parameters (numpy arrays of values). The parameters will be used in combination, using \n",
    "        itertools.product, to explore the possible configurations during the agent's decoding process. \n",
    "\n",
    "        Example format for `decoder_params`:\n",
    "            {\n",
    "                'beam_search': [{'num_beams': np.array([5, 10]), 'length_penalty': np.array([1.0, 1.5])}],\n",
    "                'sampling': [{'temperature': np.array([0.7, 1.0]), 'top_k': np.array([50, 100])}],\n",
    "                'top_p': [{'top_p': np.array([0.9, 1.0])}],\n",
    "            }\n",
    "\n",
    "        The description provides an overview of these strategies and the potential parameter combinations \n",
    "        that the agent will try during inference.\n",
    "        \"\"\"\n",
    "        return f\"Decoder Strategies: {', '.join(self.decoder_config.keys())}, with respective parameter combinations: {list(product(*[list(val.values())[0] for val in strategy])) for strategy in self.decoder_config.values()}\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': \"You didn't ring me last night . You said you would . \"},\n",
       " {'role': 'assistant',\n",
       "  'content': \" I'm sorry to have made you disappointed . \"},\n",
       " {'role': 'user',\n",
       "  'content': \" That's all right . But why were you so rude to me at lunch . \"},\n",
       " {'role': 'assistant',\n",
       "  'content': ' Was I ? Sorry , I didn ’ t mean to be . I do apologize . '},\n",
       " {'role': 'user',\n",
       "  'content': ' And why are you yarning now ? Are you bored ? '},\n",
       " {'role': 'assistant', 'content': \" Forgive me darling . I'm very tired . \"},\n",
       " {'role': 'user',\n",
       "  'content': \"What's the matter with you then ? You look miserable . \"},\n",
       " {'role': 'assistant', 'content': \" It's us . \"},\n",
       " {'role': 'user', 'content': ' What do you mean by us . '},\n",
       " {'role': 'assistant', 'content': \" Well , you always say you're busy . \"},\n",
       " {'role': 'user', 'content': \" That's right . \"},\n",
       " {'role': 'assistant',\n",
       "  'content': ' And you often go back to live with your parents and leave our son in the room by himself . '},\n",
       " {'role': 'user',\n",
       "  'content': ' I ... I ... I miss my parents , also they miss me . '},\n",
       " {'role': 'assistant',\n",
       "  'content': \" Oh I remember , I cut terrible calls , and you didn't say anything about it . \"},\n",
       " {'role': 'user', 'content': ' You mean I am groaned a few words ? '},\n",
       " {'role': 'assistant',\n",
       "  'content': \" Totally not . Perhaps it's about our marriage . \"}]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "garfield",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
